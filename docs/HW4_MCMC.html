<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Kiseok Lee" />

<meta name="date" content="2022-02-04" />

<title>HW4_MCMC</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/master/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fiveMinuteStats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/stephens999/fiveMinuteStats">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">HW4_MCMC</h1>
<h4 class="author">Kiseok Lee</h4>
<h4 class="date">2022-02-04</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2022-02-07
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>fiveMinuteStats/analysis/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.6.2). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed12345code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(12345)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed12345code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(12345)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomKiseokUchicagofiveMinuteStatstree641c8e35098981bd3c134a5e12e9b8d1902dc617targetblank641c8e3a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/KiseokUchicago/fiveMinuteStats/tree/641c8e35098981bd3c134a5e12e9b8d1902dc617" target="_blank">641c8e3</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomKiseokUchicagofiveMinuteStatstree641c8e35098981bd3c134a5e12e9b8d1902dc617targetblank641c8e3a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.
</p>
<p>
The results in this page were generated with repository version <a href="https://github.com/KiseokUchicago/fiveMinuteStats/tree/641c8e35098981bd3c134a5e12e9b8d1902dc617" target="_blank">641c8e3</a>. See the <em>Past versions</em> tab to see a history of the changes made to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/

Untracked files:
    Untracked:  texput.log

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were made to the R Markdown (<code>analysis/HW4_MCMC.Rmd</code>) and HTML (<code>docs/HW4_MCMC.html</code>) files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view the files as they were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/KiseokUchicago/fiveMinuteStats/blob/641c8e35098981bd3c134a5e12e9b8d1902dc617/analysis/HW4_MCMC.Rmd" target="_blank">641c8e3</a>
</td>
<td>
Kiseok Lee
</td>
<td>
2022-02-07
</td>
<td>
wflow_publish(“analysis/HW4_MCMC.Rmd”)
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="hw4-mcmc-markov-chain-monte-carlo-methods-metropolis-hastings-gibbs-algorithm" class="section level2">
<h2>HW4 MCMC: Markov chain monte carlo methods: Metropolis Hastings &amp; Gibbs algorithm</h2>
<p>Name: <strong>Kiseok Lee</strong><br />
Date: 2/4/22<br />
Class: <strong>HGEN 486 Computational Biology</strong></p>
</div>
<div id="a.-metropolis-hastings-algorithm" class="section level2">
<h2>A. Metropolis-Hastings Algorithm</h2>
<p><a href="https://stephens999.github.io/fiveMinuteStats/MH-examples1.html" class="uri">https://stephens999.github.io/fiveMinuteStats/MH-examples1.html</a></p>
</div>
<div id="a-1-exercise-from-example-1-sampling-from-an-exponential-distribution-using-mcmc" class="section level2">
<h2>A-1) Exercise from Example 1: sampling from an exponential distribution using MCMC</h2>
<p>First, let’s set up the Metropolis-Hastings Algorithm first</p>
<pre class="r"><code># Any MCMC scheme aims to produce
# (dependent) samples from a ``target&#39;
# distribution. In this case we are
# going to use the exponential
# distribution with mean 1 as our
# target distribution. So we start by
# defining our target density:
target = function(x) {
    if (x &lt; 0) {
        return(0)
    } else {
        return(exp(-x))
    }
}

# Having defined the function, we can
# now use it to compute a couple of
# values (just to illustrate the idea
# of a function):
target(1)</code></pre>
<pre><code>[1] 0.3678794</code></pre>
<pre class="r"><code>target(-1)</code></pre>
<pre><code>[1] 0</code></pre>
<pre class="r"><code># Next, we will program a
# Metropolis--Hastings scheme to sample
# from a distribution proportional to
# the target
x = rep(0, 1000)
x[1] = 3  #this is just a starting value, which I&#39;ve set arbitrarily to 3
for (i in 2:1000) {
    currentx = x[i - 1]
    proposedx = currentx + rnorm(1, mean = 0,
        sd = 1)
    A = target(proposedx)/target(currentx)
    if (runif(1) &lt; A) {
        x[i] = proposedx  # accept move with probabily min(1,A)
    } else {
        x[i] = currentx  # otherwise &#39;reject&#39; move, and stay where we are
    }
}

plot(x)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-2-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>hist(x)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-2-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># We can wrap this up in a function to
# make things a bit neater, and make it
# easy to try changing starting values
# and proposal distributions

easyMCMC = function(niter, startval, proposalsd) {
    x = rep(0, niter)
    x[1] = startval
    for (i in 2:niter) {
        currentx = x[i - 1]
        proposedx = rnorm(1, mean = currentx,
            sd = proposalsd)
        A = target(proposedx)/target(currentx)
        if (runif(1) &lt; A) {
            x[i] = proposedx  # accept move with probabily min(1,A)
        } else {
            x[i] = currentx  # otherwise &#39;reject&#39; move, and stay where we are
        }
    }
    return(x)
}


z1 = easyMCMC(1000, 3, 1)
plot(dexp(z1))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-2-3.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(dexp(z1, log = T))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-2-4.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>dev.off()</code></pre>
<pre><code>null device 
          1 </code></pre>
</div>
<div id="exercise-a-1-a-how-do-different-starting-values-affect-the-mcmc-scheme" class="section level2">
<h2>Exercise A-1-a) how do different starting values affect the MCMC scheme?</h2>
<p>As the starting value gets larger from 1, it takes more iterations to arrive (bigger burn-in) at exponential distribution. Therefore, if I plot the histogram to see if the distribution is similar to exponential distribution, the MH sampling starting from smaller values (&gt;1) are more closer to exponential distribution.</p>
<pre class="r"><code># Plot realizations
par(mfrow = c(3, 5))

for (i in seq(1, 50, length.out = 15)) {
    plot(easyMCMC(niter = 1000, startval = i,
        proposalsd = 1), xlab = &quot;Iterations&quot;,
        ylab = &quot;Realizations&quot;, main = paste0(&quot;Start-value= &quot;,
            i))
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-3-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Histogram
par(mfrow = c(3, 5))
for (i in seq(1, 50, length.out = 15)) {
    hist(easyMCMC(niter = 1000, startval = i,
        proposalsd = 1), probability = T,
        main = paste0(&quot;Histogram: Start-value= &quot;,
            i))
    xx = seq(0, 100, length = 1000)
    lines(xx, target(xx), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-3-2.png" width="1056" style="display: block; margin: auto;" /></p>
<p>To prove that my hypothesis is right, I can only plot the histogram with the realizations after the burn-in (only plotting x after 200 iterations). In the plot below, indeed the histogram is more closer to exponential distribution.</p>
<pre class="r"><code># Histogram
par(mfrow = c(3, 5))
for (i in seq(1, 50, length.out = 15)) {
    hist(easyMCMC(niter = 1000, startval = i,
        proposalsd = 1)[200:1000], probability = T,
        main = paste0(&quot;Histogram: Start-value= &quot;,
            i))
    xx = seq(0, 100, length = 1000)
    lines(xx, target(xx), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-4-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
</div>
<div id="exercise-a-1-b-what-is-the-effect-of-having-a-biggersmaller-proposal-standard-deviation" class="section level2">
<h2>Exercise A-1-b) what is the effect of having a bigger/smaller proposal standard deviation?</h2>
<p>As the proposal standard deviation gets bigger from 0 to 1, the MH simulations that have values closer to 1 performs better. (need less burn-in to reach exponential distribution). Extremely small standard deviations will make the realizations move too slowly.</p>
<pre class="r"><code># Plot realizations
par(mfrow = c(3, 5))

# (1) Let&#39;s start with sd below 1
for (i in seq(0, 1, length.out = 15)) {
    plot(easyMCMC(niter = 1000, startval = 1,
        proposalsd = i), xlab = &quot;Iterations&quot;,
        ylab = &quot;Realizations&quot;, main = paste0(&quot;Proposal_sd= &quot;,
            i))
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-5-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>par(mfrow = c(3, 5))
# Histogram
for (i in seq(0, 1, length.out = 15)) {
    hist(easyMCMC(niter = 1000, startval = 1,
        proposalsd = i), probability = T,
        main = paste0(&quot;Proposal_sd= &quot;, i))
    xx = seq(0, 100, length = 1000)
    lines(xx, target(xx), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-5-2.png" width="1056" style="display: block; margin: auto;" /></p>
<p>As the proposal standard deviation gets bigger from 1 to 50, the MH simulations performs worse. In many iterations, the proposal is rejected, creating a streak of same realizations throughout some periods of iterations. This is because bigger proposed value is more likely to get rejected.</p>
<pre class="r"><code># Plot realizations
par(mfrow = c(3, 5))

# (1) Let&#39;s start with sd below 1
for (i in seq(1, 50, length.out = 15)) {
    plot(easyMCMC(niter = 1000, startval = 1,
        proposalsd = i), xlab = &quot;Iterations&quot;,
        ylab = &quot;Realizations&quot;, main = paste0(&quot;Proposal_sd= &quot;,
            i))
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-6-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>par(mfrow = c(3, 5))
# Histogram
for (i in seq(1, 50, length.out = 15)) {
    hist(easyMCMC(niter = 1000, startval = 1,
        proposalsd = i), probability = T,
        main = paste0(&quot;Proposal_sd= &quot;, i))
    xx = seq(0, 100, length = 1000)
    lines(xx, target(xx), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-6-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
</div>
<div id="exercise-a-1-c-try-changing-the-target-function-to-the-following" class="section level2">
<h2>Exercise A-1-c) try changing the target function to the following</h2>
<pre class="r"><code>target = function(x) {
    return((x &gt; 0 &amp; x &lt; 1) + (x &gt; 2 &amp; x &lt;
        3))
}

easyMCMC = function(niter, startval, proposalsd) {
    x = rep(0, niter)
    x[1] = startval
    for (i in 2:niter) {
        currentx = x[i - 1]
        proposedx = rnorm(1, mean = currentx,
            sd = proposalsd)
        A = target(proposedx)/target(currentx)
        if (runif(1) &lt; A) {
            x[i] = proposedx  # accept move with probabily min(1,A)
        } else {
            x[i] = currentx  # otherwise &#39;reject&#39; move, and stay where we are
        }
    }
    return(x)
}
# see distribution
xx = seq(0, 5, length = 100)
plot(xx, target(xx))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-7-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## proposal sd = 1
plot(easyMCMC(niter = 1000, startval = 0.5,
    proposalsd = 1), xlab = &quot;Iterations&quot;,
    ylab = &quot;Realizations&quot;, main = paste0(&quot;Different target function&quot;))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-7-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>hist(easyMCMC(niter = 1000, startval = 0.5,
    proposalsd = 1), probability = T, main = paste0(&quot;Histogram for Different target function&quot;))
lines(xx, target(xx)/2, col = &quot;red&quot;)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-7-3.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## proposal sd = 0.1
plot(easyMCMC(niter = 1000, startval = 0.5,
    proposalsd = 0.1), xlab = &quot;Iterations&quot;,
    ylab = &quot;Realizations&quot;, main = paste0(&quot;Different target function&quot;))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-7-4.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>hist(easyMCMC(niter = 1000, startval = 0.5,
    proposalsd = 0.1), xlim = c(0, 4), probability = T,
    main = paste0(&quot;Histogram for Different target function&quot;))
lines(xx, target(xx)/2, col = &quot;red&quot;)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-7-5.png" width="1056" style="display: block; margin: auto;" /> When proposal sd is too small (as in 0.1 case), the realization x cannot jump from 1 region to another. Therefore, the optimal sd hyperparameter is contingent upon the sampled distribution.</p>
</div>
<div id="example-a-2-estimating-an-allele-frequency" class="section level2">
<h2>Example A-2) Estimating an allele frequency</h2>
<p>A standard assumption when modelling genotypes of bi-allelic loci (e.g. loci with alleles <span class="math inline">\(A\)</span> and <span class="math inline">\(a\)</span>) is that the population is “randomly mating”. From this assumption it follows that the population will be in “Hardy Weinberg Equilibrium” (HWE), which means that if <span class="math inline">\(p\)</span> is the frequency of the allele <span class="math inline">\(A\)</span> then the genotypes <span class="math inline">\(AA\)</span>, <span class="math inline">\(Aa\)</span> and <span class="math inline">\(aa\)</span> will have frequencies <span class="math inline">\(p^2, 2p(1-p)\)</span> and <span class="math inline">\((1-p)^2\)</span> respectively.</p>
<p>A simple prior for <span class="math inline">\(p\)</span> is to assume it is uniform on <span class="math inline">\([0,1]\)</span>. Suppose that we sample <span class="math inline">\(n\)</span> individuals, and observe <span class="math inline">\(n_{AA}\)</span> with genotype <span class="math inline">\(AA\)</span>, <span class="math inline">\(n_{Aa}\)</span> with genotype <span class="math inline">\(Aa\)</span> and <span class="math inline">\(n_{aa}\)</span> with genotype <span class="math inline">\(aa\)</span>.</p>
<p>The following R code gives a short MCMC routine to sample from the posterior distribution of <span class="math inline">\(p\)</span>. Try to go through the code to see how it works.<br />
</p>
<p>Import code first.</p>
<pre class="r"><code>prior = function(p) {
    if ((p &lt; 0) || (p &gt; 1)) {
        # || here means &#39;or&#39;
        return(0)
    } else {
        return(1)
    }
}

likelihood = function(p, nAA, nAa, naa) {
    return(p^(2 * nAA) * (2 * p * (1 - p))^nAa *
        (1 - p)^(2 * naa))
}

psampler = function(nAA, nAa, naa, niter,
    pstartval, pproposalsd) {
    p = rep(0, niter)
    p[1] = pstartval
    for (i in 2:niter) {
        currentp = p[i - 1]
        newp = currentp + rnorm(1, 0, pproposalsd)
        A = prior(newp) * likelihood(newp,
            nAA, nAa, naa)/(prior(currentp) *
            likelihood(currentp, nAA, nAa,
                naa))
        if (runif(1) &lt; A) {
            p[i] = newp  # accept move with probabily min(1,A)
        } else {
            p[i] = currentp  # otherwise &#39;reject&#39; move, and stay where we are
        }
    }
    return(p)
}</code></pre>
<p>Running this sample for <span class="math inline">\(n_{AA}\)</span> = 50, <span class="math inline">\(n_{Aa}\)</span> = 21, <span class="math inline">\(n_{aa}\)</span>=29.</p>
<pre class="r"><code>z = psampler(nAA = 50, nAa = 21, naa = 29,
    niter = 10000, pstartval = 0.5, pproposalsd = 0.01)

x = seq(0, 1, length = 1000)
hist(z, prob = T)
lines(x, dbeta(x, 122, 80))  # overlays beta density on histogram (Theoretical)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-9-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># You might also like to discard the
# first 5000 z&#39;s as &#39;burnin&#39;. Here&#39;s
# one way in R to select only the last
# 5000 z&#39;s
hist(z[5001:10000], prob = T)
lines(x, dbeta(x, 122, 80))  # overlays beta density on histogram (Theoretical)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-9-2.png" width="1056" style="display: block; margin: auto;" /></p>
<p>Investigate how the starting point and proposal standard deviation affect the convergence of the algorithm.</p>
<ol style="list-style-type: decimal">
<li>Starting point As the starting value gets larger from 0 to 0.5, it takes more iterations to arrive (bigger burn-in) at exponential distribution. Therefore, if I plot the histogram to see if the distribution is similar to beta distribution, the MCMC sampling starting from values closer to 0.5 are more closer to exponential distribution.</li>
</ol>
<pre class="r"><code># Plot realizations
par(mfrow = c(2, 4))

for (i in seq(0.01, 0.5, length.out = 8)) {
    plot(psampler(nAA = 50, nAa = 21, naa = 29,
        niter = 10000, pstartval = i, pproposalsd = 0.01),
        xlab = &quot;Iterations&quot;, ylab = &quot;Realizations&quot;,
        main = paste0(&quot;Start-value= &quot;, i))
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-10-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Histogram
par(mfrow = c(2, 4))
for (i in seq(0.01, 0.5, length.out = 8)) {
    hist(psampler(nAA = 50, nAa = 21, naa = 29,
        niter = 10000, pstartval = i, pproposalsd = 0.01),
        xlab = &quot;p&quot;, probability = T, main = paste0(&quot;Histogram: Start-value= &quot;,
            i))
    x = seq(0, 1, length = 1000)
    lines(x, dbeta(x, 122, 80), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-10-2.png" width="1056" style="display: block; margin: auto;" /></p>
<p>To prove that my hypothesis is right, I can only plot the histogram with the realizations after the burn-in (only plotting x after 1000 iterations). In the plot below, indeed the histogram is more closer to exponential distribution.</p>
<pre class="r"><code># Histogram
par(mfrow = c(2, 4))
for (i in seq(0.01, 0.5, length.out = 8)) {
    hist(psampler(nAA = 50, nAa = 21, naa = 29,
        niter = 10000, pstartval = i, pproposalsd = 0.01)[1000:10000],
        xlab = &quot;p&quot;, probability = T, main = paste0(&quot;Histogram: Start-value= &quot;,
            i))
    x = seq(0, 1, length = 1000)
    lines(x, dbeta(x, 122, 80), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-11-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Standard deviation As the proposal standard deviation gets bigger from 0 to 0.1, all the MCMC simulations except for sd=0 works. Therefore, sd as small as 0.01 still works nicely. However, when you have a sd bigger than 0.1, the rejection rate becomes higher and deviates a little from the beta distribution. Strangely, sd with 0.07 performs similarly to sd = 0.01 case.</li>
</ol>
<pre class="r"><code># Plot realizations
par(mfrow = c(2, 4))

for (i in seq(0, 0.1, length.out = 8)) {
    plot(psampler(nAA = 50, nAa = 21, naa = 29,
        niter = 10000, pstartval = 0.3, pproposalsd = i),
        xlab = &quot;Iterations&quot;, ylab = &quot;Realizations&quot;,
        main = paste0(&quot;Proposed_std= &quot;, i))
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-12-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Histogram
par(mfrow = c(2, 4))
for (i in seq(0, 0.1, length.out = 8)) {
    hist(psampler(nAA = 50, nAa = 21, naa = 29,
        niter = 10000, pstartval = 0.3, pproposalsd = i),
        xlab = &quot;p&quot;, probability = T, main = paste0(&quot;Histogram: Proposed_std= &quot;,
            i))
    x = seq(0, 1, length = 1000)
    lines(x, dbeta(x, 122, 80), col = &quot;red&quot;)
}</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-12-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
</div>
<div id="example-a-3-estimating-an-allele-frequency-and-inbreeding-coefficient" class="section level2">
<h2>Example A-3) Estimating an allele frequency and inbreeding coefficient</h2>
<p>A slightly more complex alternative than HWE is to assume that there is a tendency for people to mate with others who are slightly more closely-related than “random” (as might happen in a geographically-structured population, for example). This will result in an excess of homozygotes compared with HWE. A simple way to capture this is to introduce an extra parameter, the “inbreeding coefficient” <span class="math inline">\(f\)</span>, and assume that the genotypes <span class="math inline">\(AA\)</span>, <span class="math inline">\(Aa\)</span> and <span class="math inline">\(aa\)</span> have frequencies <span class="math inline">\(fp + (1-f)p*p, (1-f) 2p(1-p)\)</span>, and <span class="math inline">\(f(1-p) + (1-f)(1-p)(1-p)\)</span>.</p>
<p>In most cases it would be natural to treat <span class="math inline">\(f\)</span> as a feature of the population, and therefore assume <span class="math inline">\(f\)</span> is constant across loci. For simplicity we will consider just a single locus.</p>
<p>Note that both <span class="math inline">\(f\)</span> and <span class="math inline">\(p\)</span> are constrained to lie between 0 and 1 (inclusive). A simple prior for each of these two parameters is to assume that they are independent, uniform on <span class="math inline">\([0,1]\)</span>. Suppose that we sample <span class="math inline">\(n\)</span> individuals, and observe <span class="math inline">\(n_{AA}\)</span> with genotype <span class="math inline">\(AA\)</span>, <span class="math inline">\(n_{Aa}\)</span> with genotype <span class="math inline">\(Aa\)</span> and <span class="math inline">\(n_{aa}\)</span> with genotype <span class="math inline">\(aa\)</span>.</p>
<pre class="r"><code>pf_likelihood = function(p, f, nAA, nAa,
    naa) {
    return(prod((f * p + (1 - f) * p * p)^nAA,
        ((1 - f) * 2 * p * (1 - p))^nAa,
        (f * (1 - p) + (1 - f) * (1 - p) *
            (1 - p))^naa))
}

fpsampler = function(nAA, nAa, naa, niter,
    fstartval, pstartval, fproposalsd, pproposalsd) {
    f = rep(0, niter)
    p = rep(0, niter)
    f[1] = fstartval
    p[1] = pstartval
    for (i in 2:niter) {
        currentf = f[i - 1]
        currentp = p[i - 1]
        newf = currentf + rnorm(1, 0, fproposalsd)
        newp = currentp + rnorm(1, 0, pproposalsd)
        # Acceptance probability
        A = prior(newp) * pf_likelihood(p = newp,
            f = newf, nAA, nAa, naa)/(prior(currentp) *
            pf_likelihood(p = currentp, f = currentf,
                nAA, nAa, naa))
        if (runif(1) &lt; A) {
            p[i] = newp  # accept move with probabily min(1,A)
        } else {
            p[i] = currentp  # otherwise &#39;reject&#39; move, and stay where we are
        }
    }
    return(list(f = f, p = p))  # return a &#39;list&#39; with two elements named f and p
}

fp_list = fpsampler(nAA = 50, nAa = 21, naa = 29,
    niter = 10000, fstartval = 0.3, pstartval = 0.3,
    fproposalsd = 0.001, pproposalsd = 0.01)

# f
plot(fp_list$f, xlab = &quot;Iterations&quot;, ylab = &quot;f Realizations&quot;,
    main = paste0(&quot;f sampling&quot;))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-13-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># p
plot(fp_list$p, xlab = &quot;Iterations&quot;, ylab = &quot;p Realizations&quot;,
    main = paste0(&quot;p sampling&quot;))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-13-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># posterior mean
mean(fp_list$f)</code></pre>
<pre><code>[1] 3e-05</code></pre>
<pre class="r"><code>mean(fp_list$p)</code></pre>
<pre><code>[1] 0.600949</code></pre>
<pre class="r"><code># 90% credible intervals
lower = qbeta(0.05, 122, 80)
lower</code></pre>
<pre><code>[1] 0.5468983</code></pre>
<pre class="r"><code>upper = qbeta(0.95, 122, 80)
upper</code></pre>
<pre><code>[1] 0.6598491</code></pre>
<p>posterior mean of f is almost 0. posterior mean of p is about 0.6. 90% posterior credible interval is [0.547, 0.66].</p>
</div>
<div id="b.-gibbs-sampling-for-genetic-mixtures" class="section level2">
<h2>B. Gibbs sampling for genetic mixtures</h2>
<p>Read: <a href="http://stephenslab.uchicago.edu/assets/papers/Pritchard2000a.pdf.\" class="uri">http://stephenslab.uchicago.edu/assets/papers/Pritchard2000a.pdf.\</a> Consider the following simplified version of the model from Pritchard et al (2000).<br />
</p>
<p>Suppose you have a number of samples from a “population” that contains an unknown fraction of forest or savannah elephants. (Assume elephants are haploid for simplicity.) Introduce the following notation</p>
</div>
<div id="first-lets-learn-from-the-example-code" class="section level2">
<h2>First, let’s learn from the example code</h2>
<p>from <a href="https://stephens999.github.io/fiveMinuteStats/gibbs_structure_simple.html" class="uri">https://stephens999.github.io/fiveMinuteStats/gibbs_structure_simple.html</a></p>
<pre class="r"><code>set.seed(33)

# generate from mixture of normals
#&#39; @param n number of samples
#&#39; @param P a 2 by R matrix of allele frequencies
r_simplemix = function(n, P) {
    R = ncol(P)
    z = sample(1:2, prob = c(0.5, 0.5), size = n,
        replace = TRUE)  #simulate z as 1 or 2
    x = matrix(nrow = n, ncol = R)
    for (i in 1:n) {
        x[i, ] = rbinom(R, rep(1, R), P[z[i],
            ])
    }
    return(list(x = x, z = z))
}
P = rbind(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5),
    c(0.001, 0.999, 0.001, 0.999, 0.001,
        0.999))
sim = r_simplemix(n = 50, P)
x = sim$x

# Gibbs sampler code

#&#39; @param x an R vector of data
#&#39; @param P a K by R matrix of allele frequencies
#&#39; @return the log-likelihood for each of the K populations
log_pr_x_given_P = function(x, P) {
    tP = t(P)  #transpose P so tP is R by K
    return(colSums(x * log(tP) + (1 - x) *
        log(1 - tP)))
}

normalize = function(x) {
    return(x/sum(x))
}  #used in sample_z below

#&#39; @param x an n by R matrix of data
#&#39; @param P a K by R matrix of allele frequencies
#&#39; @return an n vector of group memberships
sample_z = function(x, P) {
    K = nrow(P)
    loglik_matrix = apply(x, 1, log_pr_x_given_P,
        P = P)
    lik_matrix = exp(loglik_matrix)
    p.z.given.x = apply(lik_matrix, 2, normalize)  # normalize columns
    z = rep(0, nrow(x))
    for (i in 1:length(z)) {
        z[i] = sample(1:K, size = 1, prob = p.z.given.x[,
            i], replace = TRUE)
    }
    return(z)
}


#&#39; @param x an n by R matrix of data
#&#39; @param z an n vector of cluster allocations
#&#39; @return a 2 by R matrix of allele frequencies
sample_P = function(x, z) {
    R = ncol(x)
    P = matrix(ncol = R, nrow = 2)
    for (i in 1:2) {
        sample_size = sum(z == i)
        if (sample_size == 0) {
            number_of_ones = rep(0, R)
        } else {
            number_of_ones = colSums(x[z ==
                i, ])
        }
        P[i, ] = rbeta(R, 1 + number_of_ones,
            1 + sample_size - number_of_ones)
    }
    return(P)
}

gibbs = function(x, niter = 100) {
    z = sample(1:2, nrow(x), replace = TRUE)
    res = list(z = matrix(nrow = niter, ncol = nrow(x)))
    res$z[1, ] = z

    for (i in 2:niter) {
        P = sample_P(x, z)
        z = sample_z(x, P)
        res$z[i, ] = z
    }
    return(res)
}

# Try the Gibbs sampler on the data
# simulated above.

res = gibbs(x, 100)
table(res$z[1, ], sim$z)</code></pre>
<pre><code>   
     1  2
  1 14 11
  2 10 15</code></pre>
<pre class="r"><code>table(res$z[100, ], sim$z)</code></pre>
<pre><code>   
     1  2
  1  4 26
  2 20  0</code></pre>
<pre class="r"><code>image(t(res$z))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-14-1.png" width="1056" style="display: block; margin: auto;" /></p>
</div>
<div id="another-example-code" class="section level2">
<h2>Another example code</h2>
<p>from <a href="https://stephens999.github.io/fiveMinuteStats/gibbs2.html" class="uri">https://stephens999.github.io/fiveMinuteStats/gibbs2.html</a></p>
<pre class="r"><code># To illustrate, let’s simulate data
# from this model:
set.seed(33)

# generate from mixture of normals
#&#39; @param n number of samples
#&#39; @param pi mixture proportions
#&#39; @param mu mixture means
#&#39; @param s mixture standard deviations
rmix = function(n, pi, mu, s) {
    z = sample(1:length(pi), prob = pi, size = n,
        replace = TRUE)
    x = rnorm(n, mu[z], s[z])
    return(x)
}

x = rmix(n = 1000, pi = c(0.5, 0.5), mu = c(-2,
    2), s = c(1, 1))
hist(x)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-15-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#&#39; @param x an n vector of data
#&#39; @param pi a k vector
#&#39; @param mu a k vector

mu = rnorm(k, 0, 10)</code></pre>
<pre><code>Error in rnorm(k, 0, 10): object &#39;k&#39; not found</code></pre>
<pre class="r"><code>length(mu)</code></pre>
<pre><code>Error in eval(expr, envir, enclos): object &#39;mu&#39; not found</code></pre>
<pre class="r"><code>pi</code></pre>
<pre><code>[1] 3.141593</code></pre>
<pre class="r"><code>dmat = outer(mu, x, &quot;-&quot;)</code></pre>
<pre><code>Error in outer(mu, x, &quot;-&quot;): object &#39;mu&#39; not found</code></pre>
<pre class="r"><code>dim(dmat)</code></pre>
<pre><code>Error in eval(expr, envir, enclos): object &#39;dmat&#39; not found</code></pre>
<pre class="r"><code>as.vector(pi)</code></pre>
<pre><code>[1] 3.141593</code></pre>
<pre class="r"><code>p.z.given.x = as.vector(pi) * dnorm(dmat,
    0, 1)[, 1:3]</code></pre>
<pre><code>Error in dnorm(dmat, 0, 1): object &#39;dmat&#39; not found</code></pre>
<pre class="r"><code>as.vector(pi) * dnorm(dmat, 0, 1)[, 1]</code></pre>
<pre><code>Error in dnorm(dmat, 0, 1): object &#39;dmat&#39; not found</code></pre>
<pre class="r"><code>dim(p.z.given.x)</code></pre>
<pre><code>Error in eval(expr, envir, enclos): object &#39;p.z.given.x&#39; not found</code></pre>
<pre class="r"><code>sample_z = function(x, pi, mu) {
    dmat = outer(mu, x, &quot;-&quot;)  # k by n matrix, d_kj =(mu_k - x_j)
    p.z.given.x = as.vector(pi) * dnorm(dmat,
        0, 1)
    p.z.given.x = apply(p.z.given.x, 2, normalize)  # normalize columns
    z = rep(0, length(x))
    for (i in 1:length(z)) {
        z[i] = sample(1:length(pi), size = 1,
            prob = p.z.given.x[, i], replace = TRUE)
    }
    return(z)
}

#&#39; @param z an n vector of cluster allocations (1...k)
#&#39; @param k the number of clusters
sample_pi = function(z, k) {
    counts = colSums(outer(z, 1:k, FUN = &quot;==&quot;))
    pi = gtools::rdirichlet(1, counts + 1)
    return(pi)
}

mat_o &lt;- outer(z, 1:k, FUN = &quot;==&quot;)</code></pre>
<pre><code>Error in outer(z, 1:k, FUN = &quot;==&quot;): object &#39;k&#39; not found</code></pre>
<pre class="r"><code>dim(mat_o)</code></pre>
<pre><code>Error in eval(expr, envir, enclos): object &#39;mat_o&#39; not found</code></pre>
<pre class="r"><code>counts = colSums(outer(z, 1:k, FUN = &quot;==&quot;))</code></pre>
<pre><code>Error in outer(z, 1:k, FUN = &quot;==&quot;): object &#39;k&#39; not found</code></pre>
<pre class="r"><code>counts</code></pre>
<pre><code>Error in eval(expr, envir, enclos): object &#39;counts&#39; not found</code></pre>
<pre class="r"><code>#&#39; @param x an n vector of data
#&#39; @param z an n vector of cluster allocations
#&#39; @param k the number o clusters
#&#39; @param prior.mean the prior mean for mu
#&#39; @param prior.prec the prior precision for mu
sample_mu = function(x, z, k, prior) {
    df = data.frame(x = x, z = z)
    mu = rep(0, k)
    for (i in 1:k) {
        sample.size = sum(z == i)
        sample.mean = ifelse(sample.size ==
            0, 0, mean(x[z == i]))

        post.prec = sample.size + prior$prec
        post.mean = (prior$mean * prior$prec +
            sample.mean * sample.size)/post.prec
        mu[i] = rnorm(1, post.mean, sqrt(1/post.prec))
    }
    return(mu)
}

gibbs = function(x, k, niter = 1000, muprior = list(mean = 0,
    prec = 0.1)) {
    pi = rep(1/k, k)  # initialize
    mu = rnorm(k, 0, 10)
    z = sample_z(x, pi, mu)
    res = list(mu = matrix(nrow = niter,
        ncol = k), pi = matrix(nrow = niter,
        ncol = k), z = matrix(nrow = niter,
        ncol = length(x)))
    res$mu[1, ] = mu
    res$pi[1, ] = pi
    res$z[1, ] = z
    for (i in 2:niter) {
        pi = sample_pi(z, k)
        mu = sample_mu(x, z, k, muprior)
        z = sample_z(x, pi, mu)
        res$mu[i, ] = mu
        res$pi[i, ] = pi
        res$z[i, ] = z
    }
    return(res)
}

res = gibbs(x, 2)
plot(res$mu[, 1], ylim = c(-4, 4), type = &quot;l&quot;)
lines(res$mu[, 2], col = 2)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-15-2.png" width="1056" style="display: block; margin: auto;" /></p>
</div>
<div id="adapt-the-above-code-to-our-problem" class="section level2">
<h2>Adapt the above code to our problem</h2>
<ol style="list-style-type: decimal">
<li>Simulate savannah and forest elephant data number of samples = 1000, allele frequency same as <a href="https://stephens999.github.io/fiveMinuteStats/likelihood_ratio_simple_models.html" class="uri">https://stephens999.github.io/fiveMinuteStats/likelihood_ratio_simple_models.html</a>.<br />
popoulation weight/composition = 0.3, 0.7.</li>
</ol>
<pre class="r"><code>#&#39; @param n number of samples
#&#39; @param P a 2 by R matrix of allele frequencies
#&#39; @param pi a vector with population composition, (savannah, forest)

r_simplemix = function(n,P,pi){
  R = ncol(P) # number of markers
  z = sample(1:2, prob=pi, size=n,replace=TRUE) #simulate z as 1 or 2
  x = matrix(nrow = n, ncol=R)
  for(i in 1:n){
    x[i,] = rbinom(R,rep(1,R),P[z[i],]) # get R observations from size = 1, probability as laid out in matrix P (row 1: z=1, row2: 1=2)
  }
  return(list(x=x,z=z))
}

n = 1000  # number of tusks to simulate
R = 6  # number of markers
K = 2  # number of components(populations)
P = rbind(c(0.4, 0.12, 0.21, 0.12, 0.02, 0.32), # savannah allele frequency
          c(0.8, 0.2, 0.11, 0.17, 0.23, 0.25)) # forest allele frequency
pi = c(0.4, 0.6) # population composition, (savannah, forest)

sim = r_simplemix(n=1000, P=P, pi=pi)
x = sim$x
dim(x)</code></pre>
<pre><code>[1] 1000    6</code></pre>
<pre class="r"><code># community composition
table(sim$z)</code></pre>
<pre><code>
  1   2 
401 599 </code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Implement the Gibbs sampler We assume that prior distribution of P and pi are independent. p(pi, P) = p(pi) * p(P)</li>
</ol>
<pre class="r"><code>#&#39; @param x an R vector of data
#&#39; @param P a K by R matrix of allele frequencies
#&#39; @return the log-likelihood for each of the K populations

log_pr_x_given_P = function(x, P) {
    tP = t(P)  #transpose P so tP is R by K
    return(colSums(x * log(tP) + (1 - x) *
        log(1 - tP)))
}

normalize = function(x) {
    return(x/sum(x))
}  #used in sample_z below

## Sample Z function
#&#39; @param x an n by R matrix of data
#&#39; @param P a K by R matrix of allele frequencies
#&#39; @param pi a vector with population composition, (savannah, forest)
#&#39; @return an n vector of group memberships

sample_z = function(x, pi, P) {
    K = nrow(P)
    loglik_matrix = apply(x, 1, log_pr_x_given_P,
        P = P)
    dim(loglik_matrix)
    lik_matrix = exp(loglik_matrix)
    # multiply pi
    lik_matrix = as.vector(pi) * lik_matrix
    dim(lik_matrix)
    p.z.given.x = apply(lik_matrix, 2, normalize)  # normalize columns
    z = rep(0, nrow(x))
    for (i in 1:length(z)) {
        z[i] = sample(1:K, size = 1, prob = p.z.given.x[,
            i], replace = TRUE)
    }
    return(z)
}

## Sample P function
#&#39; @param x an n by R matrix of data
#&#39; @param z an n vector of cluster allocations
#&#39; @param pi a vector with population composition, (savannah, forest)
#&#39; @return a 2 by R matrix of allele frequencies

# We assume that prior distribution of
# P and pi are independent.
sample_P = function(x, z, pi, k) {
    R = ncol(x)
    P = matrix(ncol = R, nrow = k)
    for (i in 1:k) {
        sample_size = sum(z == i)
        if (sample_size == 0) {
            number_of_ones = rep(0, R)
        } else {
            number_of_ones = colSums(x[z ==
                i, , drop = F])
        }
        P[i, ] = rbeta(R, 1 + number_of_ones,
            1 + sample_size - number_of_ones)
    }
    return(P)
}

## Sample pi function
#&#39; @param z an n vector of cluster allocations (1...k)
#&#39; @param k the number of clusters
sample_pi = function(z, k) {
    counts = colSums(outer(z, 1:k, FUN = &quot;==&quot;))
    pi = gtools::rdirichlet(1, counts + 1)
    return(pi)
}

## Gibbs sampler function

gibbs = function(x, k, niter = 1000) {
    # number of population
    print(paste0(&quot;k(number of population) is &quot;,
        k))
    # number of markers
    R = ncol(x)
    print(paste0(&quot;R(number of markers) is &quot;,
        R))
    # number of samples
    print(paste0(&quot;n(number of samples) is &quot;,
        nrow(x)))
    # initialize
    pi = rep(1/k, k)
    P = matrix(rep(0.5, k * R), nrow = k,
        ncol = R)  # initialize from uniform distribution
    z = sample_z(x, pi, P)
    # print(z)

    res = list(z = matrix(nrow = niter, ncol = nrow(x)),
        pi = matrix(nrow = niter, ncol = k),
        mat_P = matrix(nrow = niter, ncol = k *
            R))
    res$z[1, ] = z
    res$pi[1, ] = pi
    res$mat_P[1, ] = c(t(P))

    for (i in 2:niter) {
        if (i%%100 == 0) {
            print(paste0(&quot;niter: &quot;, i, &quot;, pi: &quot;,
                pi))
        }
        pi = sample_pi(z, k)
        P = sample_P(x, z, pi, k)
        z = sample_z(x, pi, P)
        res$z[i, ] = z
        res$pi[i, ] = pi
        # print(paste0(&#39;niter: &#39;,i,&#39;,
        # pi: &#39;,pi))
        res$mat_P[i, ] = c(t(P))
    }
    return(res)
}


# Try the Gibbs sampler on the data
# simulated above.

res = gibbs(x, k = 2, niter = 1000)</code></pre>
<pre><code>[1] &quot;k(number of population) is 2&quot;
[1] &quot;R(number of markers) is 6&quot;
[1] &quot;n(number of samples) is 1000&quot;
[1] &quot;niter: 100, pi: 0.417431154528918&quot; &quot;niter: 100, pi: 0.582568845471082&quot;
[1] &quot;niter: 200, pi: 0.555711944701249&quot; &quot;niter: 200, pi: 0.444288055298751&quot;
[1] &quot;niter: 300, pi: 0.775309210221034&quot; &quot;niter: 300, pi: 0.224690789778966&quot;
[1] &quot;niter: 400, pi: 0.709718617886303&quot; &quot;niter: 400, pi: 0.290281382113697&quot;
[1] &quot;niter: 500, pi: 0.858445238358121&quot; &quot;niter: 500, pi: 0.141554761641879&quot;
[1] &quot;niter: 600, pi: 0.662652409539676&quot; &quot;niter: 600, pi: 0.337347590460324&quot;
[1] &quot;niter: 700, pi: 0.336069572111723&quot; &quot;niter: 700, pi: 0.663930427888277&quot;
[1] &quot;niter: 800, pi: 0.528990659096347&quot; &quot;niter: 800, pi: 0.471009340903653&quot;
[1] &quot;niter: 900, pi: 0.44787964792192&quot; &quot;niter: 900, pi: 0.55212035207808&quot;
[1] &quot;niter: 1000, pi: 0.598839765621071&quot; &quot;niter: 1000, pi: 0.401160234378929&quot;</code></pre>
</div>
<div id="evaluate-the-gibbs-sampling" class="section level2">
<h2>Evaluate the gibbs sampling</h2>
<pre class="r"><code># (1) Pi is it converging? 
plot(res$pi[,1], xlab=&quot;iteration&quot;, ylab=&quot;Component proportion&quot;, col=&#39;red&#39;, type = &quot;b&quot;,lty = 1, pch=19,lwd=1, main=&quot;MCMC (Gibbs)&quot;, ylim = c(0, 1))
lines(res$pi[,2], col=&#39;blue&#39;, type = &quot;b&quot;,lty = 1, pch=19,lwd=1)
abline(h = 0.4, col = &#39;red&#39;, lty=3, lwd=2)
abline(h = 0.6, col = &#39;blue&#39;, lty=3, lwd=2)
legend(&quot;bottomright&quot;, legend=c(&#39;Savannah&#39;, &#39;Forest&#39;, &#39;Truth&#39;), col=c(&quot;Red&quot;, &quot;Blue&quot;, &quot;Black&quot;), lty=c(1,1,3), cex=0.8, box.lty=0)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-18-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># (2) Accuracy
table(res$z[1,],sim$z)</code></pre>
<pre><code>   
      1   2
  1 209 287
  2 192 312</code></pre>
<pre class="r"><code>table(res$z[100,],sim$z)</code></pre>
<pre><code>   
      1   2
  1 222 173
  2 179 426</code></pre>
<pre class="r"><code>table(res$z[200,],sim$z)</code></pre>
<pre><code>   
      1   2
  1 272 303
  2 129 296</code></pre>
<pre class="r"><code>table(res$z[300,],sim$z)</code></pre>
<pre><code>   
      1   2
  1 345 429
  2  56 170</code></pre>
<pre class="r"><code>table(res$z[1000,],sim$z)</code></pre>
<pre><code>   
      1   2
  1 300 304
  2 101 295</code></pre>
<pre class="r"><code>vec_accuracy = rep(-1, dim(res$z)[1])
niter = dim(res$z)[1]
for (i in 1:dim(res$z)[1]){
  t_iter = table(res$z[i,],sim$z)
  vec_accuracy[i] = (t_iter[1,1] + t_iter[2,2]) / sum(t_iter)
}

plot(vec_accuracy, xlab=&quot;iteration&quot;, ylab=&quot;Accuracy (right Z prediction / number_of_samples)&quot;, col=&#39;red&#39;, type = &quot;b&quot;,lty = 1, pch=19,lwd=1, main=&quot;MCMC (Gibbs) and accuracy&quot;, ylim = c(0, 1))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-18-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># (3) P (allele frequency)
# res$mat_P[niter,]
vec_allele &lt;- c(paste0(&quot;F0_&quot;,1:6),paste0(&quot;F1_&quot;,1:6))

P = rbind(c(0.4, 0.12, 0.21, 0.12, 0.02, 0.32), # savannah allele frequency
          c(0.8, 0.2, 0.11, 0.17, 0.23, 0.25))
vec_true &lt;- c(t(P))
df_freq &lt;- rbind(data.frame(Allele=vec_allele, Frequency = vec_true, Data=&quot;True_value&quot;),
                 data.frame(Allele=vec_allele, Frequency = res$mat_P[niter,], Data=&quot;Prediction&quot;))

ggplot(df_freq, aes(x=Allele, y=Frequency, group=Data, color=Data)) +
  geom_line(size = 2) +
  # scale_fill_manual(values=cols) +
  ylab(&quot;Allele frequency \n&quot;) +
  xlab(&quot;\n f k_j: Allele position j of population k&quot;) +
  scale_y_continuous(limits=c(0,1),breaks=seq(0,1,0.1))+
  ggtitle(&quot;True allele frequency and prediction \n&quot;)+
  # geom_text(aes(label = round(Freq,3)), size = 3, vjust = -1.5, family=&quot;serif&quot;, show.legend = FALSE, position = position_dodge(0.8))+
  mytheme_2d</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-18-3.png" width="1056" style="display: block; margin: auto;" /></p>
</div>
<div id="extra-points-increase-k-to-arbitrary-number-between-3-to-5." class="section level2">
<h2>Extra points: increase k to arbitrary number between 3 to 5.</h2>
<pre class="r"><code>r_sim_arbitraryK = function(n, R) {
    # n = number of samples, R = number
    # of markers, pi = population
    # composition pick k
    k = sample(3:5, 1)
    # generate P
    P = matrix(nrow = k, ncol = R)
    for (j in 1:k) {
        # print(j)
        frequency = runif(R, 0, 1)
        P[j, ] = frequency
    }
    # generate pi
    pi = runif(k)
    pi = pi/sum(pi)

    # sample z
    z = sample(1:k, prob = pi, size = n,
        replace = TRUE)  #simulate z as 1 or 2
    x = matrix(nrow = n, ncol = R)
    for (i in 1:n) {
        x[i, ] = rbinom(R, rep(1, R), P[z[i],
            ])  # get R observations from size = 1, probability as laid out in matrix P (row 1: z=1, row2: 1=2)
    }
    return(list(x = x, z = z, k = k, P = P,
        pi = pi))
}

n = 1000  # number of tusks to simulate
R = 6  # number of markers

arb_sim = r_sim_arbitraryK(n = 1000, R = 6)
x_arb = arb_sim$x
dim(x_arb)</code></pre>
<pre><code>[1] 1000    6</code></pre>
<pre class="r"><code>arb_sim$z</code></pre>
<pre><code>   [1] 5 5 4 5 5 5 4 5 5 4 4 1 1 5 5 2 5 1 5 1 5 1 5 4 1 4 4 4 5 2 5 5 4 5 5 4 5
  [38] 4 5 1 4 5 4 1 4 4 5 4 5 2 2 5 4 5 4 2 4 4 2 4 5 4 4 4 5 2 5 4 5 1 2 4 3 5
  [75] 5 5 4 2 2 2 5 4 4 5 1 5 5 4 4 4 4 5 2 1 4 4 4 5 4 4 3 4 5 2 5 4 5 4 5 1 3
 [112] 4 5 4 4 4 4 4 2 4 4 2 4 4 4 4 5 3 4 3 5 5 5 3 4 1 5 2 2 2 5 4 5 2 4 4 2 4
 [149] 5 5 4 2 1 3 4 5 2 4 1 5 2 4 5 4 1 4 5 4 2 2 4 1 4 2 4 4 4 4 2 5 4 4 4 4 2
 [186] 4 4 5 1 2 4 2 5 5 5 4 3 4 1 1 2 4 3 5 5 2 5 4 1 2 5 5 2 4 4 5 2 5 4 4 4 1
 [223] 4 5 4 2 5 5 4 4 5 4 4 5 5 2 2 4 4 4 4 3 4 5 5 2 1 5 4 5 2 1 5 1 4 1 5 1 5
 [260] 5 5 5 1 5 5 4 1 5 5 5 2 4 2 5 5 5 4 5 5 4 5 5 5 5 4 4 5 5 5 2 4 1 5 5 2 4
 [297] 4 2 4 1 1 4 4 4 4 4 2 4 5 4 4 5 4 4 5 1 2 1 5 3 2 5 5 4 5 4 4 5 5 5 5 4 3
 [334] 1 4 1 2 4 1 1 4 1 5 4 4 5 5 2 5 4 2 5 5 5 5 2 5 5 4 4 4 4 4 1 3 5 5 1 4 5
 [371] 3 1 5 5 4 5 2 5 2 5 5 4 4 2 2 1 5 4 4 1 5 4 5 5 1 5 2 4 4 5 1 4 1 4 5 5 5
 [408] 4 4 4 2 4 1 5 4 4 1 4 2 5 5 5 4 4 4 4 5 1 5 4 5 4 2 5 5 2 1 4 4 1 5 4 4 4
 [445] 5 1 4 4 5 4 5 4 2 4 4 5 4 5 3 5 4 5 5 4 2 4 1 5 4 4 4 4 2 4 1 4 5 5 4 4 1
 [482] 5 4 4 5 4 3 2 5 3 5 4 5 4 5 4 4 4 4 5 2 2 4 5 5 4 5 2 4 4 5 2 5 4 4 3 4 5
 [519] 2 4 5 5 1 4 5 5 4 4 1 5 4 5 5 5 4 4 1 2 4 2 2 5 5 4 5 5 1 4 4 4 4 5 4 5 4
 [556] 4 4 4 4 4 4 5 5 5 5 2 4 4 5 5 1 5 5 1 5 5 5 5 3 5 4 4 4 4 4 4 1 3 4 5 5 2
 [593] 2 5 1 5 4 4 2 5 3 1 5 4 5 1 2 5 5 5 5 5 4 5 5 4 5 4 1 5 5 4 1 5 4 1 5 4 4
 [630] 5 4 4 4 5 1 4 1 4 5 4 4 5 4 2 5 3 4 2 4 2 4 4 2 1 5 4 1 5 4 2 5 3 5 5 4 5
 [667] 5 1 2 4 5 1 4 5 5 1 4 5 5 5 4 5 5 5 5 5 4 4 1 4 1 4 4 5 5 4 4 5 5 4 5 2 5
 [704] 3 1 2 5 5 4 5 5 4 5 1 5 5 5 4 3 4 5 1 1 1 4 5 4 2 5 4 2 3 4 5 5 4 4 4 4 5
 [741] 4 5 2 5 4 1 5 4 5 4 5 1 4 5 5 4 4 4 5 4 4 1 5 2 5 1 4 4 4 5 4 2 4 5 4 5 4
 [778] 5 5 5 5 4 4 5 4 2 5 5 2 4 2 3 2 4 3 4 2 1 5 4 1 4 1 5 4 5 5 5 5 5 5 5 4 2
 [815] 4 5 4 4 1 5 4 1 5 1 2 2 5 2 5 4 2 4 4 4 4 2 5 4 4 5 5 2 5 3 4 2 5 2 4 5 5
 [852] 5 2 2 4 4 4 2 5 5 4 1 4 3 4 5 2 5 1 5 2 4 4 5 2 5 5 1 2 5 5 4 4 5 4 1 1 5
 [889] 5 3 4 2 4 5 1 4 5 5 5 5 4 2 1 1 1 5 4 4 1 5 2 4 4 2 1 4 4 4 4 5 4 2 4 5 5
 [926] 5 4 4 4 1 4 5 1 2 1 5 1 1 1 3 4 1 4 1 4 2 2 2 5 2 2 5 2 4 5 2 1 2 4 5 4 2
 [963] 5 5 5 1 4 1 5 1 5 5 1 1 5 4 4 5 4 1 4 4 3 4 5 5 4 5 1 1 4 4 5 2 1 4 2 5 1
[1000] 5</code></pre>
<pre class="r"><code>arb_sim$k</code></pre>
<pre><code>[1] 5</code></pre>
<pre class="r"><code>arb_sim$pi</code></pre>
<pre><code>[1] 0.13648067 0.11938209 0.03191186 0.37171326 0.34051212</code></pre>
<pre class="r"><code>arb_sim$P</code></pre>
<pre><code>          [,1]      [,2]      [,3]       [,4]      [,5]       [,6]
[1,] 0.4207590 0.6244815 0.6205796 0.06378931 0.8523212 0.04626184
[2,] 0.2773479 0.4265226 0.3119986 0.97131908 0.0196396 0.07738109
[3,] 0.1739043 0.7116329 0.2067251 0.70708675 0.1601342 0.17575030
[4,] 0.5380617 0.1669658 0.2939560 0.84420595 0.6224021 0.05199443
[5,] 0.8496468 0.3465639 0.7996776 0.41435821 0.1184500 0.34025380</code></pre>
<pre class="r"><code># community composition
table(arb_sim$z)</code></pre>
<pre><code>
  1   2   3   4   5 
123 129  33 364 351 </code></pre>
<p>Gibbs simulation</p>
<pre class="r"><code>res_arb = gibbs(x_arb, k = 5, niter = 1000)</code></pre>
<pre><code>[1] &quot;k(number of population) is 5&quot;
[1] &quot;R(number of markers) is 6&quot;
[1] &quot;n(number of samples) is 1000&quot;
[1] &quot;niter: 100, pi: 0.114855844394583&quot;  &quot;niter: 100, pi: 0.0801449292347373&quot;
[3] &quot;niter: 100, pi: 0.399264207060853&quot;  &quot;niter: 100, pi: 0.205137370841093&quot; 
[5] &quot;niter: 100, pi: 0.200597648468734&quot; 
[1] &quot;niter: 200, pi: 0.126089182370777&quot;  &quot;niter: 200, pi: 0.109341722095568&quot; 
[3] &quot;niter: 200, pi: 0.420997799578577&quot;  &quot;niter: 200, pi: 0.303985466871874&quot; 
[5] &quot;niter: 200, pi: 0.0395858290832049&quot;
[1] &quot;niter: 300, pi: 0.139816230260457&quot;  &quot;niter: 300, pi: 0.0286287303024365&quot;
[3] &quot;niter: 300, pi: 0.432222996151119&quot;  &quot;niter: 300, pi: 0.294949262013021&quot; 
[5] &quot;niter: 300, pi: 0.104382781272967&quot; 
[1] &quot;niter: 400, pi: 0.155997200218159&quot;  &quot;niter: 400, pi: 0.0682877138165943&quot;
[3] &quot;niter: 400, pi: 0.408346628524271&quot;  &quot;niter: 400, pi: 0.248399210203577&quot; 
[5] &quot;niter: 400, pi: 0.1189692472374&quot;   
[1] &quot;niter: 500, pi: 0.164476265963827&quot;  &quot;niter: 500, pi: 0.0228405684329271&quot;
[3] &quot;niter: 500, pi: 0.447000600719267&quot;  &quot;niter: 500, pi: 0.302162915130182&quot; 
[5] &quot;niter: 500, pi: 0.0635196497537971&quot;
[1] &quot;niter: 600, pi: 0.0132534632827818&quot; &quot;niter: 600, pi: 0.0642644423783572&quot;
[3] &quot;niter: 600, pi: 0.462463288457102&quot;  &quot;niter: 600, pi: 0.331948054377619&quot; 
[5] &quot;niter: 600, pi: 0.12807075150414&quot;  
[1] &quot;niter: 700, pi: 0.016848810109853&quot; &quot;niter: 700, pi: 0.312952064590725&quot;
[3] &quot;niter: 700, pi: 0.293218604991052&quot; &quot;niter: 700, pi: 0.267921422366061&quot;
[5] &quot;niter: 700, pi: 0.10905909794231&quot; 
[1] &quot;niter: 800, pi: 0.0424752655667812&quot; &quot;niter: 800, pi: 0.232602312180554&quot; 
[3] &quot;niter: 800, pi: 0.206388428718395&quot;  &quot;niter: 800, pi: 0.335876755395304&quot; 
[5] &quot;niter: 800, pi: 0.182657238138967&quot; 
[1] &quot;niter: 900, pi: 0.0649213890851309&quot; &quot;niter: 900, pi: 0.208277473725405&quot; 
[3] &quot;niter: 900, pi: 0.330922286780376&quot;  &quot;niter: 900, pi: 0.253748184881022&quot; 
[5] &quot;niter: 900, pi: 0.142130665528067&quot; 
[1] &quot;niter: 1000, pi: 0.0848812010058377&quot; &quot;niter: 1000, pi: 0.130206300245794&quot; 
[3] &quot;niter: 1000, pi: 0.317458841997871&quot;  &quot;niter: 1000, pi: 0.335160539672143&quot; 
[5] &quot;niter: 1000, pi: 0.132293117078355&quot; </code></pre>
<p>Evaluate the gibbs sampling</p>
<pre class="r"><code># (1) Pi is it converging?
plot(res_arb$pi[, 1], xlab = &quot;iteration&quot;,
    ylab = &quot;Component proportion&quot;, col = &quot;red&quot;,
    type = &quot;b&quot;, lty = 1, pch = 19, lwd = 1,
    main = &quot;MCMC (Gibbs)&quot;, ylim = c(0, 1))
lines(res_arb$pi[, 2], col = &quot;blue&quot;, type = &quot;b&quot;,
    lty = 1, pch = 19, lwd = 1)
lines(res_arb$pi[, 3], col = &quot;green&quot;, type = &quot;b&quot;,
    lty = 1, pch = 19, lwd = 1)

abline(h = arb_sim$pi[1], col = &quot;red&quot;, lty = 3,
    lwd = 2)
abline(h = arb_sim$pi[2], col = &quot;blue&quot;, lty = 3,
    lwd = 2)
abline(h = arb_sim$pi[3], col = &quot;green&quot;,
    lty = 3, lwd = 2)

legend(&quot;bottomright&quot;, legend = c(&quot;Pop1&quot;,
    &quot;Pop2&quot;, &quot;Pop3&quot;, &quot;Truth&quot;), col = c(&quot;Red&quot;,
    &quot;Blue&quot;, &quot;Green&quot;, &quot;Black&quot;), lty = c(1,
    1, 1, 3), cex = 0.8, box.lty = 0)</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-21-1.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># (2) Accuracy
table(res_arb$z[1, ], arb_sim$z)</code></pre>
<pre><code>   
     1  2  3  4  5
  1 14 23  8 77 70
  2 32 22  5 77 70
  3 20 31  8 76 55
  4 31 25  3 67 77
  5 26 28  9 67 79</code></pre>
<pre class="r"><code>table(res_arb$z[100, ], arb_sim$z)</code></pre>
<pre><code>   
      1   2   3   4   5
  1  72   2   3  33  13
  2  11  34   9  17  13
  3  12  68  11 233  54
  4  23  13   3  58 128
  5   5  12   7  23 143</code></pre>
<pre class="r"><code>table(res_arb$z[200, ], arb_sim$z)</code></pre>
<pre><code>   
      1   2   3   4   5
  1  81   4   5  21  26
  2   9  38   8  36  19
  3  15  68  12 262  56
  4  17  12   4  42 226
  5   1   7   4   3  24</code></pre>
<pre class="r"><code>table(res_arb$z[300, ], arb_sim$z)</code></pre>
<pre><code>   
      1   2   3   4   5
  1  77   3   5  30  25
  2   0   3   1   9  14
  3  15  87  14 272  70
  4  17   9   4  35 215
  5  14  27   9  18  27</code></pre>
<pre class="r"><code>table(res_arb$z[1000, ], arb_sim$z)</code></pre>
<pre><code>   
      1   2   3   4   5
  1  11   1   1   8  74
  2   2  48   7  49  26
  3  22  54  10 247  52
  4  21  25  10  40 190
  5  67   1   5  20   9</code></pre>
<pre class="r"><code>vec_accuracy = rep(-1, dim(res_arb$z)[1])
niter = dim(res_arb$z)[1]
for (i in 1:dim(res_arb$z)[1]) {
    t_iter = table(res_arb$z[i, ], arb_sim$z)
    vec_accuracy[i] = sum(diag(t_iter))/sum(t_iter)
}

plot(vec_accuracy, xlab = &quot;iteration&quot;, ylab = &quot;Accuracy (right Z prediction / number_of_samples)&quot;,
    col = &quot;red&quot;, type = &quot;b&quot;, lty = 1, pch = 19,
    lwd = 1, main = &quot;MCMC (Gibbs) and accuracy&quot;,
    ylim = c(0, 1))</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-21-2.png" width="1056" style="display: block; margin: auto;" /></p>
<pre class="r"><code># (3) P (allele frequency)
# res_arb$mat_P[niter,]
K = arb_sim$k
R = 6

# name of the allele
vec_allele &lt;- c()
for (i in 1:K) {
    vec_allele &lt;- c(vec_allele, rep(paste0(&quot;F&quot;,
        i), R))
}

vec_allele &lt;- paste0(vec_allele, &quot;_&quot;, rep(1:R,
    K))

vec_true &lt;- c(t(arb_sim$P))
df_freq &lt;- rbind(data.frame(Allele = vec_allele,
    Frequency = vec_true, Data = &quot;True_value&quot;),
    data.frame(Allele = vec_allele, Frequency = res_arb$mat_P[niter,
        ], Data = &quot;Prediction&quot;))

ggplot(df_freq, aes(x = Allele, y = Frequency,
    group = Data, color = Data)) + geom_line(size = 2) +
    # scale_fill_manual(values=cols) +
ylab(&quot;Allele frequency \n&quot;) + xlab(&quot;\n f k_j: Allele position j of population k&quot;) +
    scale_y_continuous(limits = c(0, 1),
        breaks = seq(0, 1, 0.1)) + ggtitle(&quot;True allele frequency and prediction \n&quot;) +
    # geom_text(aes(label =
    # round(Freq,3)), size = 3, vjust =
    # -1.5, family=&#39;serif&#39;, show.legend
    # = FALSE, position =
    # position_dodge(0.8))+
mytheme_2d</code></pre>
<p><img src="figure/HW4_MCMC.Rmd/unnamed-chunk-21-3.png" width="1056" style="display: block; margin: auto;" /></p>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 4.0.3 (2020-10-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19042)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] rmarkdown_2.9      ggpubr_0.4.0       ggrepel_0.9.1      ape_5.5           
 [5] openxlsx_4.2.3     devtools_2.4.0     usethis_2.0.1      gtools_3.8.2      
 [9] reshape2_1.4.4     readxl_1.3.1       magrittr_2.0.1     forcats_0.5.1     
[13] stringr_1.4.0      purrr_0.3.4        readr_1.4.0        tidyr_1.1.3       
[17] tibble_3.0.4       tidyverse_1.3.1    vegan_2.5-7        lattice_0.20-41   
[21] permute_0.9-5      RColorBrewer_1.1-2 ggplot2_3.3.5      dplyr_1.0.5       
[25] knitr_1.37        

loaded via a namespace (and not attached):
 [1] colorspace_2.0-0  ggsignif_0.6.2    ellipsis_0.3.2    rio_0.5.27       
 [5] rprojroot_2.0.2   fs_1.5.0          rstudioapi_0.13   farver_2.1.0     
 [9] remotes_2.4.0     fansi_0.4.2       lubridate_1.7.10  xml2_1.3.2       
[13] splines_4.0.3     cachem_1.0.4      pkgload_1.2.1     jsonlite_1.7.2   
[17] workflowr_1.6.2   broom_0.7.9       cluster_2.1.0     dbplyr_2.1.1     
[21] compiler_4.0.3    httr_1.4.2        backports_1.2.1   assertthat_0.2.1 
[25] Matrix_1.2-18     fastmap_1.1.0     cli_3.0.1         formatR_1.11     
[29] later_1.2.0       htmltools_0.5.1.1 prettyunits_1.1.1 tools_4.0.3      
[33] gtable_0.3.0      glue_1.4.2        Rcpp_1.0.5        carData_3.0-4    
[37] cellranger_1.1.0  jquerylib_0.1.4   vctrs_0.3.8       nlme_3.1-149     
[41] xfun_0.29         ps_1.6.0          testthat_3.0.2    rvest_1.0.1      
[45] lifecycle_1.0.0   rstatix_0.7.0     MASS_7.3-53       scales_1.1.1     
[49] hms_1.1.0         promises_1.2.0.1  parallel_4.0.3    yaml_2.2.1       
[53] curl_4.3.2        memoise_2.0.0     sass_0.4.0        stringi_1.5.3    
[57] highr_0.9         desc_1.3.0        pkgbuild_1.2.0    zip_2.1.1        
[61] rlang_0.4.10      pkgconfig_2.0.3   evaluate_0.14     tidyselect_1.1.1 
[65] processx_3.5.1    plyr_1.8.6        R6_2.5.0          generics_0.1.0   
[69] DBI_1.1.1         pillar_1.6.0      haven_2.4.1       whisker_0.4      
[73] foreign_0.8-80    withr_2.4.2       mgcv_1.8-33       abind_1.4-5      
[77] modelr_0.1.8      crayon_1.4.1      car_3.0-11        utf8_1.1.4       
[81] grid_4.0.3        data.table_1.14.0 callr_3.7.0       git2r_0.28.0     
[85] reprex_2.0.0      digest_0.6.27     httpuv_1.6.0      munsell_0.5.0    
[89] bslib_0.2.5.1     sessioninfo_1.1.1</code></pre>
</div>
</div>

<hr>
<p>
    This site was created with <a href="http://rmarkdown.rstudio.com">R Markdown</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
